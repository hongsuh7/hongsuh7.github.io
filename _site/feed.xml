<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-09T14:57:24-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">In All Probably</title><subtitle>A collection of math topics with lots of pictures and animations!
</subtitle><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><entry><title type="html">A Generalized Elo System for Tennis Players, part 1</title><link href="http://localhost:4000/2020/07/07/tennis-1.html" rel="alternate" type="text/html" title="A Generalized Elo System for Tennis Players, part 1" /><published>2020-07-07T00:00:00-07:00</published><updated>2020-07-07T00:00:00-07:00</updated><id>http://localhost:4000/2020/07/07/tennis-1</id><content type="html" xml:base="http://localhost:4000/2020/07/07/tennis-1.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Here, we will create an Elo rating system for male tennis players and do some analysis on the choice of parameters. We find that a random choice of parameters actually does quite well, and that a wide variety of K-factor weight functions do a good job predicting the outcomes of tennis matches. In future notebooks, we will further expand upon this model by adding various features.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;All of the code I wrote for this project is &lt;a href=&quot;https://github.com/hongsuh7/tennis-elo/blob/master/elo1.ipynb&quot;&gt;here&lt;/a&gt;, with similar text.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;The Elo rating system is designed to assign ratings to players which reflect their true strength at the game. Players are assigned an initial rating of 1. Before a match between two players, the system outputs a probability that one wins. After the match, the system updates each player’s rating according to their previous ratings and the result of the match.&lt;/p&gt;

&lt;h3 id=&quot;elos-rules&quot;&gt;Elo’s rules&lt;/h3&gt;
&lt;p&gt;Define ++\sigma(x) = \exp(x) / (\exp(x) + 1),++ the logistic function, and let $K$ be a constant. If player one (p1) has rating $x$ and player two (p2) has rating $y$, the probability that p1 wins is given by $\sigma(x-y)$. Suppose $w=1$ if p1 wins and $w=0$ if p1 loses. After the match, the ratings are updated with the rule ++x \mapsto x + (-1)^{w+1} \sigma((-1)^w(x-y)),\quad y \mapsto y+(-1)^w \sigma((-1)^w (x-y)).++&lt;/p&gt;

&lt;h3 id=&quot;deriving-elos-rules&quot;&gt;Deriving Elo’s rules&lt;/h3&gt;
&lt;p&gt;Let’s see why this makes any sense. Suppose we are starting from scratch and we want to develop this kind of rating system, but have no idea what the win probability and the update rule should be. Heuristically, it makes sense to suppose that $W,U$ are functions of the difference $x-y$ rather than the separate ratings $x,y$. Let $W(x-y)$ be the probability that p1 wins against p2, and let $U(x-y)$ be the update rule if p1 loses and $U(y-x)$ be the update rule if p1 wins, so that ++ x\mapsto x+(-1)^{w+1} U((-1)^w(x-y)),\quad y\mapsto y+(-1)^w U((-1)^w(x-y)).++ We need $W$ and $U$ to satisfy some basic rules.&lt;/p&gt;

&lt;p&gt;First, $W(x-y) = 1-W(y-x)$ so ++W(z)+W(-z)=1.++&lt;/p&gt;

&lt;p&gt;Second, ++\lim_{x\to \infty} W(x)=1 ~\text{ and }~ \lim_{x\to-\infty} U(x) = 0.++&lt;/p&gt;

&lt;p&gt;Third and last, &lt;em&gt;the expected update of both players must be zero&lt;/em&gt;. This is because the strengths of the players shouldn’t actually change after a match, so the Elo rating (which is supposed to reflect true strength) shouldn’t do that. Here, $w$ is the random quantity in question (make sure to distinguish between little $w$ and capital $W$: the relationship is that $W(x-y)$ should estimate $P(w=1)$). Since the expected update of both players equals zero, we have ++ W(x-y) \cdot U(y-x) - W(y-x) \cdot U(x-y) = 0. ++ In other words, ++ \frac{W(z)}{W(-z)} = \frac{U(z)}{U(-z)}.++
In fact, these are the only real mathematical requirements for such a rating system. (See &lt;a href=&quot;https://www.stat.berkeley.edu/~aldous/Papers/me-Elo-SS.pdf&quot;&gt;Aldous&lt;/a&gt; for more details and assumptions.) As we see, an easy choice would be to set $KW=U$, where $K$ is any constant greater than zero. And really we may set $W$ to be any cdf, but logistic is the standard choice.&lt;/p&gt;

&lt;p&gt;In practice, the choice of $K$ is quite important, and the constant is called the &lt;em&gt;K-factor&lt;/em&gt;, not to be confused with the tennis racquet line. This $K$ is the main subject of this note, and we will refer to it many times.&lt;/p&gt;

&lt;p&gt;Generally $K$ is taken to be a decreasing function of the number of matches that the updating player has played. In our case, following FiveThirtyEight’s family of functions for $K$, we take ++K(n) = \frac{a}{(b+n)^c},++ where $n$ is the number of matches played by that player before the match. We will call $K(n)$ the &lt;strong&gt;K-factor function&lt;/strong&gt;. All of the analysis of parameters we do below focus on the parameters $a,b,c$ above.&lt;/p&gt;

&lt;p&gt;We will use &lt;a href=&quot;http://jeffsackmann.com&quot;&gt;Jeff Sackmann&lt;/a&gt;’s immense trove of tennis data; see his &lt;a href=&quot;https://github.com/JeffSackmann&quot;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;differences-with-standard-elo-ratings&quot;&gt;Differences with standard Elo ratings&lt;/h3&gt;
&lt;p&gt;You may notice that I chose to work with the natural base rather than base-10 with some weird 400 multiplicative factor like with standard Elo ratings. The only difference between my ratings and the ratings you may see on FiveThirtyEight or Jeff Sackmann’s website is a multiplicative factor of $400/\ln 10\approx 173.72$. There is also an additive factor, but we can ignore that because Elo ratings give the same predictions when the same value is added to all ratings (and all initial ratings). We explain the origin of the multiplicative factor below.&lt;/p&gt;

&lt;p&gt;For the standard Elo rating, the win probability of player with rating $x$ (who is facing a player with rating $y$) is ++ \frac{1}{1+10^{(y-x)/400}} = \frac{1}{1+ \exp\left(\frac{\ln 10}{400}(y-x)\right)} ++
So we have to multiply our ratings by $400/\ln 10\approx 173.72$ to get the standard Elo ratings. There’s also an additive factor of $1000-173.72$ because generally the tennis ratings start at 1000, and we start at 1.&lt;/p&gt;

&lt;h2 id=&quot;cost-functions&quot;&gt;Cost functions&lt;/h2&gt;
&lt;p&gt;There are many ways to measure the accuracy of a prediction model which outputs probabilities. Let $n$ be the number of matches being analyzed and $p_i$ the win probability that the model assigns to the winner for $i=1,2,\ldots,n$.&lt;/p&gt;

&lt;p&gt;The easiest to understand is &lt;em&gt;win prediction rate&lt;/em&gt;, which is simply the proportion of matches for which the model assigns a probability greater than 0.5 to the winner. In the code, win prediction rate is denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt;. Below, $\mathbb{1}(A)=1$ if $A$ happens and $\mathbb{1}(A)=0$ if $A$ does not happen. Technically this is not a cost function, it’s a profit function. Take the negative of this if you want a cost function.
++\text{wp} = n^{-1}\sum_{i=1}^n \mathbb{1}\{p_i &amp;gt; 0.5\}.++&lt;/p&gt;

&lt;p&gt;Next, we introduce &lt;em&gt;log-loss error&lt;/em&gt;, appearing in maximum likelihood estimation and logistic regression. In the code, log-loss error is denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;.
++\text{ll} = -n^{-1}\sum_{i=1}^n \log p_i.++&lt;/p&gt;

&lt;p&gt;Finally, we introduce the &lt;em&gt;Brier score&lt;/em&gt;, which is simply the mean squared error between $p_i, i=1,2,\ldots,n$ and the perfect data which assigns probability one to the winner every time. In the code, Brier score is denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;.
++\text{bs} = n^{-1} \sum_{i=1}^n (1-p_i)^2.++&lt;/p&gt;

&lt;p&gt;Here are some apparent differences between these cost/profit functions.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;win prediction rate is not continuous (a small change in probabilities can change 0s to 1s and 1s to 0s) and also flat in many places (with respect to the parameters), so it is less useful for optimization than the other two functions, though still a nice metric.&lt;/li&gt;
  &lt;li&gt;log-loss is not bounded, while Brier score is. To see the difference, suppose $p_1=10^{-100}$. Then this contributes 230 to the log-loss error, while it contributes 1 to the Brier score. So log-loss is less tolerant of wrongly confident predictions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s first look at FiveThirtyEight’s model, described in a very nice article about &lt;a href=&quot;https://fivethirtyeight.com/features/serena-williams-and-the-difference-between-all-time-great-and-greatest-of-all-time/&quot;&gt;Serena Williams’ dominance over the years&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fivethirtyeights-model&quot;&gt;FiveThirtyEight’s model&lt;/h2&gt;
&lt;p&gt;FiveThirtyEight’s parameter choices were ++[a,b,c]=[250/174, 5, 0.4] \approx [1.44, 5, 0.4].++ Let’s test their model on 2014 data with one year of history and 34 years of history.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;One year of history:
log-loss=0.601, win pred=0.650, brier score=0.209
many years of history:
log-loss=0.585, win pred=0.669, brier score=0.201
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I got the same log-loss error that &lt;a href=&quot;https://core.ac.uk/download/pdf/86638968.pdf&quot;&gt;Kovalchik&lt;/a&gt; got, but I got a different win prediction. I’m not entirely certain how that happened, since I think we used the same data set. I’m gonna have to go on with what I have.&lt;/p&gt;

&lt;h2 id=&quot;sensitivity-of-ratings&quot;&gt;Sensitivity of Ratings&lt;/h2&gt;
&lt;p&gt;Recall that the parameters we are interested in are the ones which dictate the behavior of the &lt;em&gt;K-factor function&lt;/em&gt;. My main observation for this post is that ratings are not very sensitive to the parameters because 1) the model itself is pretty robust, and 2) there are some redundancies in our three parameters. By this, I mean that very different sets of parameters can produce similar K-factor functions.&lt;/p&gt;

&lt;p&gt;We will initialize an object of class Elo with parameters $p$ which are obtained from the following distribution:&lt;/p&gt;

&lt;p&gt;++ p \sim \text{Unif}([0,2]\times[2,5]\times[0,1]). ++&lt;/p&gt;

&lt;p&gt;We will calculate and plot log-loss error, win prediction rate, and Brier score for 100 models with random parameters drawn from the above distribution. To do this fast, we need to modify our class to accommodate &lt;strong&gt;vectorized&lt;/strong&gt; operations. We change our code so that ratings according to different parameters are all updated simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tennis-1/ll.png&quot; alt=&quot;1&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/wp.png&quot; alt=&quot;2&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/bs.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The conclusion is that &lt;strong&gt;most parameters drawn from this simple uniform distribution do pretty well.&lt;/strong&gt; The parameter population is the densest where the best costs are achieved. In other words, good parameters are not rare. Recall that log-loss=0.585, win pred=0.669, brier score=0.201 were the costs achieved by the handpicked FiveThirtyEight parameters. The plots have vertical lines at these checkpoints.&lt;/p&gt;

&lt;h2 id=&quot;you-bring-your-best-fifty-ill-bring-mine&quot;&gt;You bring your best fifty, I’ll bring mine&lt;/h2&gt;
&lt;p&gt;How good are the best parameters from these randomly chosen ones? Let’s pick the top 1% of parameters and see how they perform. I will pick the 50 best parameters (tested on 2010-2013) in each of the three categories from 5000 random ones with 1990-2013 data. Then we test them on 2014 data. For each category, to get a single probability, we took the 50 probabilities generated by the 50 models and averaged them.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  optimized_for        ll        wp        bs
0            ll  0.581776  0.670516  0.199910
1            wp  0.595646  0.666440  0.204376
2            bs  0.581727  0.669497  0.199868
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that the parameters which were obtained by optimizing for log-loss and Brier score were about equally effective at predicting 2014 matches, and the win-prediction-optimized parameters were less effective. But overall, all of these parameters are &lt;em&gt;not bad&lt;/em&gt;. What do these parameters look like? Are they clustered anywhere? Are they all over the place? Recall the roles of the first, second, and third parameters, below denoted by $a,b,c$: ++k(n) = \frac{a}{(b+n)^c}.++&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tennis-1/ab.png&quot; alt=&quot;4&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/ac.png&quot; alt=&quot;5&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/bc.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From these plots, we can see that basically b doesn’t matter, and based on the type of error optimized for, the parameters cluster around different pairs of $(a,c)$ values. There is some overlap between the log-loss optimized parameters and the Brier-score optimized parameters, as evidenced by the purple points.&lt;/p&gt;

&lt;p&gt;The correlation in the second plot is due to the fact that, for a fixed $n,a,b,$ and $c$, the set of points $(x,y)$ satisfying ++\frac{x}{(b+n)^y} = \frac{a}{(b+n)^c}++ is a log curve.&lt;/p&gt;

&lt;p&gt;In short, &lt;strong&gt;there are a variety of parameter choices that can lead to effective predictions.&lt;/strong&gt; All of the parameters shown above achieve pretty good error rates.&lt;/p&gt;

&lt;p&gt;You may be wondering if maybe, two sets of parameters could be quite different as points in 3-d space but give rise to two very similar k-factor functions $k(n)$. This is true, as evidenced by the clustering around a log curve shown above. Our tentative conclusion is that parameters within a single group (that is, optimized for the same cost function) give rise to similar K-factor functions, but the three different groups (&lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;-optimized,&lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt;-optimized,&lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;-optimized) give rise to fairly different K-factor functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tennis-1/k.png&quot; alt=&quot;7&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/k2.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems clear that the differently colored K-factor functions behave quite differently, both near zero and at infinity.&lt;/p&gt;

&lt;h2 id=&quot;testing-on-2015-2019-data&quot;&gt;Testing on 2015-2019 data&lt;/h2&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;-optimized and &lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;-optimized parameters (trained on 1980-2013) performed a tiny better than the FiveThirtyEight model with respect to each of the three cost functions for 2014 data. What about for 2015-2019 data? We’ll check it out here.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  optimized_for        ll        wp        bs
0            ll  0.607112  0.658860  0.209942
1            wp  0.623405  0.657555  0.214121
2            bs  0.607411  0.658723  0.209912
3           538  0.611903  0.661607  0.210791
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;FiveThirtyEight takes the highest win prediction rate. On the other hand, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;- and &lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;-optimized parameters achieve the best log-loss errors. Overall, aside from &lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt;-optimized parameters, all of these models seem to give roughly equally effective predictions. (This discrepancy is explained by the fact that mathematically, &lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt; is not nice to work with because it is flat and discontinuous.)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;First, this analysis casts doubt on my initial feeling that there is one “true” K-factor function that we wish to approximate. It seems that many quite different K-factor functions can be used for effective and near-optimal prediction, given this model.&lt;/p&gt;

&lt;p&gt;Second, I believe that a random choice of parameters is a better one than a handpicked one, from an algorithmic point of view. It separates the parameters from most human fiddling (except the distribution that we pick from, which was pretty generic – a uniform distribution on a large 3-d box).&lt;/p&gt;

&lt;p&gt;Third, the fact that many different K-factor functions work opens up a new frontier of prediction with Elo. Can we use, say, ten different Elo models at once to make a better prediction than any one of them can? Perhaps we can use a fast-updating Elo model in conjunction with a slow-updating Elo model to generate more effective predictions. We will explore these ideas and more in the next notebook.&lt;/p&gt;</content><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><summary type="html">Introduction Here, we will create an Elo rating system for male tennis players and do some analysis on the choice of parameters. We find that a random choice of parameters actually does quite well, and that a wide variety of K-factor weight functions do a good job predicting the outcomes of tennis matches. In future notebooks, we will further expand upon this model by adding various features.</summary></entry><entry><title type="html">Simulated Annealing and Smitten Ice Cream</title><link href="http://localhost:4000/2020/06/29/smitten.html" rel="alternate" type="text/html" title="Simulated Annealing and Smitten Ice Cream" /><published>2020-06-29T00:00:00-07:00</published><updated>2020-06-29T00:00:00-07:00</updated><id>http://localhost:4000/2020/06/29/smitten</id><content type="html" xml:base="http://localhost:4000/2020/06/29/smitten.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I live in Oakland, about a mile away from a &lt;a href=&quot;https://www.smittenicecream.com&quot;&gt;Smitten Ice Cream&lt;/a&gt; store. Their selling point is their super-fast liquid nitrogen made-to-order ice cream. They claim that the ice cream, which is turned solid from liquid in 90 seconds, is creamier than regular ice cream. The validity of the scientific basis of this claim, I can’t answer, but I can make a simple mathematical model, derived from physical principles, to simulate the comparison between Smitten-made ice cream and regular ice cream.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Of course, I wasn’t really thinking about Smitten when I was learning this stuff (I prefer &lt;a href=&quot;https://www.curbsideoakland.com&quot;&gt;Curbside&lt;/a&gt;). I was looking for ways to optimize a pretty rough function that was not really suitable for any gradient techniques, and I wanted to do something a little more sophisticated than literally a random search over some portion of the parameter space (which ended up being the best option, I’ll post about this later). A friend told me about &lt;a href=&quot;https://en.wikipedia.org/wiki/Simulated_annealing&quot;&gt;simulated annealing&lt;/a&gt;, which seemed right up my alley because I did a little statistical mechanics in grad school. It ended up not working really well for my problem, but I did find this topic interesting so I thought I’d write about it. &lt;strong&gt;My goal in this post is to explain a simple model for flash freezing using simulated annealing.&lt;/strong&gt; I was inspired by the image on the Wikipedia page, which you should compare to the images in this post.&lt;/p&gt;

&lt;h2 id=&quot;particle-configurations&quot;&gt;Particle Configurations&lt;/h2&gt;

&lt;p&gt;Let’s consider an arrangement of two types of particles in a box in a $n\times n$ two-dimensional lattice, like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine that these particles form the liquid that Smitten ice cream is made from. Of course, the actual Smitten Goo in all probably has millions of different kinds of particles, not arranged in a grid, in three-dimensional space, whatever. Our purpose here is to simplify the very complicated reality of ice cream as much as we can until we arrive at the simplest model which demonstrates the essence of our question: &lt;em&gt;creaminess&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To do this, we only need two types of particles. We’ll assign red particles to the value $+1$ and blue particles to the value $-1$. We will often say that red particles have &lt;em&gt;positive spin&lt;/em&gt; and blue particles have &lt;em&gt;negative spin&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The entire arrangement of particles is called a &lt;em&gt;particle configuration&lt;/em&gt;. To reiterate and simplify, a particle configuration consists of an assignment of $+1$ or $-1$ to each site in the $n\times n$ box. We will denote the collection of all particles configurations by $B_n$. So each element in $B_n$ is a different particle configuration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;energy-and-temperature&quot;&gt;Energy and temperature&lt;/h2&gt;

&lt;p&gt;Energy and temperature are two different quantities which work against, and for, one another to accomplish the task of freezing, whcih we are interested in because, remember, we love ice cream.&lt;/p&gt;

&lt;h3 id=&quot;energy&quot;&gt;Energy&lt;/h3&gt;

&lt;p&gt;Here is the part where we invoke some physical principles. Each particle configuration has an energy associated to it. The details of energy are not pressing right now, so we will defer this discussion to the appendix. You can invent any notion of energy you’d like. We describe one type of energy, &lt;em&gt;attractive energy&lt;/em&gt;, below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Particles want to position themselves in a low-energy configuration. This is a general physical principle, the Second Law of Thermodynamics. In our model, their ability to do so depends on one parameter, &lt;em&gt;temperature&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;temperature&quot;&gt;Temperature&lt;/h3&gt;

&lt;p&gt;Here is another physically guided principle, though I’m not sure what the following tendency is called. At high temperatures, particles are more tolerant of high-energy configurations. As the temperature decreases, particles develop a more urgent need to have low energy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-simulated-annealing-algorithm&quot;&gt;The simulated annealing algorithm&lt;/h3&gt;

&lt;p&gt;So far, we have described what particles would like to do, but we haven’t specified a process with which they can accomplish their desires. We will begin to describe the simulated annealing algorithm, which does exactly this. Essentially, &lt;strong&gt;simulated annealing simulates the movement of particles in the process of freezing.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To simulate the process of freezing, we let a particle configuration evolve over time in the following way. At each time interval, one particle is randomly chosen. Then it either switches its spin or keeps its spin, depending on the &lt;strong&gt;energy&lt;/strong&gt; of the configuration and the &lt;strong&gt;temperature&lt;/strong&gt; of the environment.&lt;/p&gt;

&lt;p&gt;Here is the qualitative evolution rule:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Begin with a random initial configuration.&lt;/li&gt;
  &lt;li&gt;Record the energy $E$.&lt;/li&gt;
  &lt;li&gt;At each time interval, choose a random particle, switch its spin, and recompute the energy $E’$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The particle configuration either accepts or declines the change. The choice is random, with tendencies listed below.&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt; &lt;/th&gt;
          &lt;th&gt;high temperature&lt;/th&gt;
          &lt;th&gt;low temperature&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$E&amp;gt;E’$&lt;/td&gt;
          &lt;td&gt;slight inclination to accept&lt;/td&gt;
          &lt;td&gt;strong inclination to accept&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;$E&amp;lt;E’$&lt;/td&gt;
          &lt;td&gt;slight inclination to decline&lt;/td&gt;
          &lt;td&gt;strong inclination to decline&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;Once the change is either accepted or declined, the temperature decreases by some amount.&lt;/li&gt;
  &lt;li&gt;Repeat steps 2 through 5 for a fixed number of iterations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is not the exact physical process by which freezing of liquids happens. This is just a simple mathematical model which captures the creaminess property that we wish to investigate. This is the simulated annealing algorithm. It was invented for the sake of finding global extrema of a general class of functions. But that’s not what we’re using it for today.&lt;/p&gt;

&lt;h2 id=&quot;flash-freezing&quot;&gt;Flash Freezing&lt;/h2&gt;

&lt;p&gt;Here’s the interesting part. &lt;strong&gt;The speed of temperature decrease affects the microscopic structure of the particle configuration&lt;/strong&gt;. If we compare two simulated annealing processes with different temperature decrease speeds and identical initial configurations, initial temperatures, final temperatures, and number of iterations, then the slow-cooled configuration will have more ‘‘order’’ than the fast-cooled configuration. We will see different examples of this phenomenon below. Below, we see slow-cooled configurations and fast-cooled configurations with identical parameters otherwise, for two different notions of energy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/6_3.png&quot; alt=&quot;6&quot; /&gt;
&lt;img src=&quot;/assets/smitten-1/7_2.png&quot; alt=&quot;7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the first example, slow cooling results in larger groups of same-spin particles than fast cooling does. In the second example, slow cooling results in a more crystalline arrangement of particles than fast cooling does. This is what I mean when I say that slow cooling produces more order than fast cooling. The reason is that generally, more order means less energy, and slow cooling results in a lower final energy than fast cooling does. (When using simulated annealing for an optimization problem, fast cooling will find a local minimum close to the starting point, and slow cooling will get you closer to the global minimum.)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The freezing of ice cream is a much more complicated physical process than the simple model described above. However, this simple model captures many aspects of the ice cream freezing process. In particular, we see that a slow freezing process results in a crystalline structure while a fast freezing process results in a disordered liquidlike structure. Ice crystal formation follows a similar energy principle to the one described above, and fast freezing an ice cream goo is sure to generate smaller crystals, which are more disorderly, than slow freezing. See the image below to see what flash freezing does to food (?) cells (obtained from flash-freeze.net):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/cells.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The flash-frozen cells retain cell structure because the ice crystals are smaller and less crystalline, which results in a smaller volume expansion (remember that ice is less dense than water), which prevents cells from exploding. Overall, flash-freezing a liquid keeps it more similar to the original liquid than slow-freezing it does. So there is a mathematical basis to Smitten’s claim that their 90-second liquid-nitrogen-induced ice cream making process produces smaller ice crystals and therefore a creamier texture than other ice creams.&lt;/p&gt;

&lt;p&gt;To be clear, though, I don’t really detect any creaminess difference between Smitten and other ice creams. Especially when a Smitten pint is eleven dollars…&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;p&gt;I considered periodic grids, so that neighbors are easy to calculate. The energy I used for the first simulation above is a standard Ising model energy
++ E(x) = -\sum_{\|i-j\|_{\infty} = 1} x_i x_j ++ where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$i,j \in \mathbb{Z}^2$;&lt;/li&gt;
  &lt;li&gt;$x_i \in \{+1,-1\}$ is the spin at site $i$; and&lt;/li&gt;
  &lt;li&gt;$\|i-j\|_{\infty}$ is the larger of the vertical and horizontal distances between $i$ and $j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The energy I used for the second simulation includes a repulsive energy at larger distances&lt;/p&gt;

&lt;p&gt;++ E(x) = -\sum_{\|i-j\|_{\infty} = 1} x_i x_j + \frac{1}{2} \sum_{\|i-j\|_\infty=3} x_i x_j. ++&lt;/p&gt;

&lt;p&gt;The temperature decrease speed I used was of the following shape, with speed parameter $\alpha$
++ T(\alpha, t) = 4(1-t)^\alpha + 1, ++ where $t$ goes from zero to one. The slow cooling implemented a linear decrease with $\alpha=1$ and the parameter for fast cooling was $\alpha = 10$.&lt;/p&gt;

&lt;p&gt;A few more details:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the energies you should normalize by dividing by the size of the grid;&lt;/li&gt;
  &lt;li&gt;the images are 100 by 100;&lt;/li&gt;
  &lt;li&gt;the first pair of images went through 300000 time steps and the second pair through 100000 time steps;&lt;/li&gt;
  &lt;li&gt;this takes impossibly long if you recalculate the entire energy after a sign flip, so you have to calculate the local change to update the energy;&lt;/li&gt;
  &lt;li&gt;the switch-acceptance probability was ++ P(E, E’, T) = \frac{1}{Z}\exp((E - E’)/ T), ++ where $E$ is old energy, $E’$ is new energy, $T\in [0,1]$ is temperature, and $Z$ is a normalizing constant which I set to be one. Note that this quantity, though it is called a probability, is not exactly a probability because it may be larger than one. Just cut it off at one.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

Since this is the first post, suffice it to say this: we will pick a particular probability distribution on $B_n$ such that *lower-energy particle configurations have higher probability of occurring*. 

Furthermore, the probability distribution has a parameter $\beta$ between 0 and infinity. Increasing $\beta$ makes lower-energy particle configurations **even more likely**. Decreasing $\beta$ makes lower-energy particle configurations **a little less likely than they were with a higher $\beta$**.
--&gt;

&lt;!-- next up let's apply simulated annealing to find least-cost path across potentials
http://eprints.qut.edu.au/62208/1/MiaoTianV11withPubInfo.pdf --&gt;</content><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><summary type="html">Introduction I live in Oakland, about a mile away from a Smitten Ice Cream store. Their selling point is their super-fast liquid nitrogen made-to-order ice cream. They claim that the ice cream, which is turned solid from liquid in 90 seconds, is creamier than regular ice cream. The validity of the scientific basis of this claim, I can’t answer, but I can make a simple mathematical model, derived from physical principles, to simulate the comparison between Smitten-made ice cream and regular ice cream.</summary></entry></feed>