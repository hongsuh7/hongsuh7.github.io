<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-13T01:00:16-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">In All Probably</title><subtitle>A collection of math topics with lots of pictures and animations!
</subtitle><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><entry><title type="html">Testing Robustness of Neural ODEs against Adversarial Attacks</title><link href="http://localhost:4000/2020/07/22/neural-ode-robustness.html" rel="alternate" type="text/html" title="Testing Robustness of Neural ODEs against Adversarial Attacks" /><published>2020-07-22T00:00:00-07:00</published><updated>2020-07-22T00:00:00-07:00</updated><id>http://localhost:4000/2020/07/22/neural-ode-robustness</id><content type="html" xml:base="http://localhost:4000/2020/07/22/neural-ode-robustness.html">&lt;p&gt;In my &lt;a href=&quot;https://hongsuh7.github.io/2020/07/17/neural-ode-intro.html&quot;&gt;last post&lt;/a&gt;, I wrote an introduction to Neural ODEs. I made a simple network with one processing layer, one ODE layer, and one fully connected layer. In this post, I will compare the robustness of this model (which we call an “ODENet”) to regular ResNets. We see a statistically significant improvement against adversarial attacks when switching from ResNets to ODENets. However, the difference is quite small. This exploration was informed by the paper &lt;a href=&quot;https://arxiv.org/pdf/1910.05513.pdf&quot;&gt;On Robustness of Neural Ordinary Differential Equations&lt;/a&gt; (Yan et al).&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;Code for this blog post is on my &lt;a href=&quot;https://github.com/hongsuh7&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compromises-and-opinions&quot;&gt;Compromises and Opinions&lt;/h2&gt;

&lt;p&gt;Recall that I said it took six hours for me to train the simple ODENet I described in the previous post. I couldn’t do meaningful analyses if one data point takes me half a day to compute. So I decided to approximate the ODENet.&lt;/p&gt;

&lt;p&gt;I approximated it in basically the simplest way possible: the regular ResNet is an explicit Euler method with step size 1; I used the explicit Euler method with step size 1/4. &lt;strong&gt;Philosophical tangent:&lt;/strong&gt; I think there is actually little reason for anyone to consider implicit methods, adaptive/small step sizes, or higher-order methods for ODENets. Why?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Implicit methods are better than explicit ones when it comes to complicated ODEs with “stiff”, or steep, $f$s in $\dot{x} = f(x,t)$. There is no evidence to me that a ResNet block is like this at all. For example, in the original ResNet paper, the blocks basically look like ++ \text{ReLU} \circ \text{conv} \circ \text{ReLU} \circ \text{conv}  ++ and this doesn’t seem sufficiently high-gradient enough to justify using implicit methods. In the paper &lt;a href=&quot;https://arxiv.org/abs/1811.00995&quot;&gt;Invertible Residual Networks&lt;/a&gt;, the authors (many of them also wrote the Neural ODEs paper) constrain these blocks to have Lipschitz constant less than one, and this does not affect performance in a significant manner; this suggests to me that even without constraints, these blocks (by themselves) don’t have huge Lipschitz constants and therefore doesn’t necessitate using implicit methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As I said, the &lt;a href=&quot;https://arxiv.org/abs/1811.00995&quot;&gt;Invertible Residual Networks&lt;/a&gt; paper suggests to me that the residual blocks have fairly small Lipschitz constants. So large step sizes with explicit methods should approximate the ODE solution fairly well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;And after all, there is no “true” solution we are trying to approximate with discrete steps in the first place. So there is no reason to think that (prior to the experiments) larger error of the numerical method will lead to poorer performance of the classifier.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I do think that the injectivity of the ODE map helps with something. Yan et al. think that this property of the ODE map is responsible for its adversarial robustness. But we don’t need tiny step sizes for this to happen, and I don’t think implicit methods really help. For example, if $f$ has Lipschitz constant $L$, then $f/(2L)$ has Lipschitz constant $1/2$, so we need only to do explicit Euler method with step size $1/\lceil 2L \rceil$ in order to get injectivity. If you have opinions on this, please let me know. I’m curious to see what people think.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;I replaced the ODEBlock $\dot{x} = f(x), ~ t\in[0,1]$ in the previous post with the explicit Euler method with step size 1/4. I compared this with the corresponding ResNet, which can be seen as the explicit Euler method with step size 1. I conducted Gaussian noise tests (adding Gaussian noise with the indicated standard deviation to the entire image) and FGSM (fast gradient sign method) tests. I also conducted PGD (projected gradient descent) attacks but they were basically zero all the time for like epsilon = 0.15 so I didn’t include them. The summary statistics are below. The following is R code.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; nrow(euler1)
[1] 15

&amp;gt; nrow(euler4)
[1] 11

&amp;gt; sapply(X = euler1, FUN = mean)
    plain   gaus0.5   gaus1.0   gaus1.5  fgsm0.15   fgsm0.3   fgsm0.5 
98.146667 93.480667 67.374000 41.986667 57.528000 16.608000  1.681333 

&amp;gt; sapply(X = euler1, FUN = sd)
     plain    gaus0.5    gaus1.0    gaus1.5   fgsm0.15    fgsm0.3    fgsm0.5 
 0.1836404  2.1582515  5.5016579  4.4575805 27.3154483 13.2658871  2.8893545 

&amp;gt; sapply(X = euler4, FUN = mean)
    plain   gaus0.5   gaus1.0   gaus1.5  fgsm0.15   fgsm0.3   fgsm0.5 
98.331818 93.660000 65.752727 40.272727 63.420000 18.394545  1.952727

&amp;gt; sapply(X = euler4, FUN = sd)
     plain    gaus0.5    gaus1.0    gaus1.5   fgsm0.15    fgsm0.3    fgsm0.5 
 0.1373913  2.3716281  5.2053032  3.0574568 23.4010983 18.3954023  3.9513772 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the standard deviations! What the summary statistics don’t tell you is that the distribution of numbers for the fgsm tests were fairly bimodal; more than half the time, these models do very poorly on the fgsm tests (like &amp;lt;5% on fgsm0.3) and the other times, they did a lot better (like &amp;gt;20% on fgsm0.5). You can take a look at the dataset on my &lt;a href=&quot;https://github.com/hongsuh7&quot;&gt;GitHub&lt;/a&gt;. Carrying out a statistical test is almost meaningless here, because obviously there is no statisically significant difference between the two methods.&lt;/p&gt;

&lt;p&gt;I still don’t know why the distribution is bimodal on adversarial tests. I’d love to find out—please let me know if you know why this happens.&lt;/p&gt;

&lt;h2 id=&quot;adversarial-training-results&quot;&gt;Adversarial Training: Results&lt;/h2&gt;

&lt;p&gt;To really see the potential for robustness in these models, we should do some adversarial training before testing. To do this, with each mini-batch I made my usual gradient descent step, then I also calculated the corresponding fgsm attacks with epsilon=0.15. I made another gradient descent step with these perturbed images and their corresponding labels, but the loss function I multiplied by 0.5 (to tell the model that the original images are more important than the perturbed ones, but not by much).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; nrow(euler1_adv)
[1] 24

&amp;gt; nrow(euler4_adv)
[1] 14

&amp;gt; sapply(euler1_adv, mean)
   plain  gaus0.5  gaus1.0  gaus1.5 fgsm0.15  fgsm0.3  fgsm0.5  fgsm0.7 
98.44625 97.19875 75.43000 44.59667 95.14208 86.97542 60.76250 26.09750 

&amp;gt; sapply(euler1_adv, sd)
    plain   gaus0.5   gaus1.0   gaus1.5  fgsm0.15   fgsm0.3   fgsm0.5   fgsm0.7 
0.1407376 0.4559111 4.5464013 4.7227900 0.6125001 1.8283872 6.3299538 8.5499999 

&amp;gt; sapply(euler4_adv, mean)
   plain  gaus0.5  gaus1.0  gaus1.5 fgsm0.15  fgsm0.3  fgsm0.5  fgsm0.7 
98.61857 97.49929 74.84214 44.54429 95.72857 88.15000 59.82071 26.48643 

&amp;gt; sapply(euler4_adv, sd)
    plain   gaus0.5   gaus1.0   gaus1.5  fgsm0.15   fgsm0.3   fgsm0.5   fgsm0.7 
0.1029456 0.6656687 4.3496387 3.5756306 0.3303811 1.6361399 6.7798326 7.5182287 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ok, now we’re cooking. The variances are down – in fact they are very small for fgsm0.15 and fgsm0.3. We can do a regular t-test with the fgsm data.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; t.test(euler1_adv$fgsm0.15, euler4_adv$fgsm0.15)

	Welch Two Sample t-test

data:  euler1_adv$fgsm0.15 and euler4_adv$fgsm0.15
t = -3.8317, df = 35.875, p-value = 0.000493
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.8969503 -0.2760259
sample estimates:
mean of x mean of y 
 95.14208  95.72857 

&amp;gt; t.test(euler1_adv$fgsm0.3, euler4_adv$fgsm0.3)

	Welch Two Sample t-test

data:  euler1_adv$fgsm0.3 and euler4_adv$fgsm0.3
t = -2.0431, df = 29.877, p-value = 0.04995
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.3488747386 -0.0002919281
sample estimates:
mean of x mean of y 
 86.97542  88.15000 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The 4-step Euler network is statistically significantly better against adversarial attacks than the 1-step Euler network! But, as you can see, it’s not &lt;em&gt;that&lt;/em&gt; much better. The other two attacks, fgsm0.5 and fgsm0.7, have no statistically significant difference.&lt;/p&gt;

&lt;p&gt;We can also do t-tests on the other columns. 4-step Euler is definitely better than 1-step Euler for the plain (no perturbation) tests, whether adversarially trained or not.&lt;/p&gt;

&lt;p&gt;Note that the adversarially trained networks are better than the regularly trained networks even with no attacks, and with Gaussian noise attacks.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We see that even with this simple structure, we see a statistically significant improvement in the 4-step Euler network from the 1-step Euler network (regular ResNet). It does take more computational cost to backprop through more functions, though the number of parameters is equal. But yes, we do conclude that ODENets are more robust against adversarial attacks, and a little better in general, than regular ResNets (but not by much).&lt;/p&gt;</content><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><summary type="html">In my last post, I wrote an introduction to Neural ODEs. I made a simple network with one processing layer, one ODE layer, and one fully connected layer. In this post, I will compare the robustness of this model (which we call an “ODENet”) to regular ResNets. We see a statistically significant improvement against adversarial attacks when switching from ResNets to ODENets. However, the difference is quite small. This exploration was informed by the paper On Robustness of Neural Ordinary Differential Equations (Yan et al).</summary></entry><entry><title type="html">A Simple Introduction to Neural ODEs</title><link href="http://localhost:4000/2020/07/17/neural-ode-intro.html" rel="alternate" type="text/html" title="A Simple Introduction to Neural ODEs" /><published>2020-07-17T00:00:00-07:00</published><updated>2020-07-17T00:00:00-07:00</updated><id>http://localhost:4000/2020/07/17/neural-ode-intro</id><content type="html" xml:base="http://localhost:4000/2020/07/17/neural-ode-intro.html">&lt;p&gt;You might have heard some hype around &lt;a href=&quot;https://arxiv.org/pdf/1806.07366.pdf&quot;&gt;neural ordinary differential equations&lt;/a&gt; (Neural ODEs). The idea of using a continuous process to model a discrete one is very fundamental in many parts of mathematics, so I was excited to learn about this stuff. Even though the code from the original paper is available online, I couldn’t find a simple high-level explanation + implementation of neural ODEs on a simple dataset. In this post, I’ll explain the idea behind and purported advantages of Neural ODEs and create a MNIST classifier using a Neural ODE.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Recall that a resnet (residual neural network) is simply a neural network with residual blocks in them. To explain residual blocks, let $x_0\in \mathbb{R}^m$ be the input to the network and $f_0,f_1,\ldots, f_{n-1}: \mathbb{R}^m \to \mathbb{R}^m$ be functions whose second argument takes in some parameters $\alpha_i$. Then a residual block is the map $x_0 \mapsto x_n$ below:
++ \begin{cases} x_0&amp;amp;\mapsto x_0 + f_0(x_0, \alpha_0) = x_1, \\ x_1 &amp;amp;\mapsto x_1 + f_1(x_1, \alpha_1) = x_2, \\ &amp;amp;\vdots \\ x_{n-1} &amp;amp;\mapsto x_{n-1} + f_{n-1}(x_{n-1}, \alpha_{n-1}) = x_n. \end{cases}++
Note that a particularity of this structure is that the dimension must remain the same at each step, which is why all the functions $f_i$ must map $\mathbb{R}^m \to \mathbb{R}^m$.&lt;/p&gt;

&lt;p&gt;Rewriting the residual block, we see that it is a discrete difference equation with control $\alpha$:
++ \begin{cases} x_1 - x_0 &amp;amp;= f_0(x_0,\alpha_0), \\ x_2 - x_1 &amp;amp;= f_1(x_1,\alpha_1), \\ &amp;amp;\vdots \\ x_n - x_{n-1} &amp;amp;= f_{n-1}(x_{n-1}, \alpha_{n-1}). \end{cases} ++
When I say that $\alpha$ is the control, I mean that it is a sort of external value you can manipulate to change the behavior of the difference equation.&lt;/p&gt;

&lt;p&gt;If we move the subscripts of $f$ into the third argument of $f$, then we can write the residual block as the map
++ x_0 \mapsto x_n, \text{ where }\quad x_{k+1} - x_k = f(x_k, \alpha_k, k). ++&lt;/p&gt;

&lt;p&gt;Writing it this way, we see that a residual block has a natural continuous analogue. Consider $x_0,x_1,\ldots,x_n$ as the evolution of a state through time, where $0,1,\ldots,n$ is considered the passing of time. Then the map from $x_0$ to $x_1$ is simply the evolution of the state $x_0$ for 1 unit of time. Similarly, the map from $x_0$ to $x_n$ is the evolution of the initial state $x_0$ for $n$ units of time. Each line in the residual block can be seen as an approximation of a time-one differential equation map (which is the map taking $x(0)$ to $x(1)$ if $x$ is the solution of an ODE), and we can string them all together into a time-$n$ differential equation map
++ x_0 \mapsto x(n), \text{ where }\quad \begin{cases} \dot{x}(t) &amp;amp;= f(x(t), \alpha(t), t), \\ x(0) &amp;amp;= x_0. \end{cases} ++&lt;/p&gt;

&lt;p&gt;This is the idea of Neural ODEs: replace residual blocks with their continuous analogue. The claim is that there are two advantages.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The continuous process might be better at modeling the truth than the discrete process.&lt;/li&gt;
  &lt;li&gt;Increasing the number of layers increases the memory usage of backprop. There is a natural way to obtain the gradient of the loss function with respect to the parameters with ODEs called the &lt;em&gt;adjoint method&lt;/em&gt; which uses less memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;a-note-on-the-adjoint-method&quot;&gt;A note on the adjoint method&lt;/h3&gt;
&lt;p&gt;The idea of the adjoint method is the following. Suppose initial time is zero and final time is $T$. Suppose the state space is $\mathbb{R}^m$.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Solve the ODE forward in time to get the solution $x(T)$ at final time. No need to store any information in this forward pass.&lt;/li&gt;
  &lt;li&gt;Compute the backwards adjoint ODE with the terminal state $x(T)$ going back to the initial time $t=0$. The solution is labeled $a(t) \in \mathbb{R}^m, t \in [0,T]$, and we do not need to store this data. To compute $a(t)$ backwards, we simultaneously recompute $x(t)$ backwards.&lt;/li&gt;
  &lt;li&gt;Recall from step 2 that we are not storing the $a(t)$ information. Instead, we can compute the gradient of the loss function with respect to the parameters by adding up some function of $a(t)$ backwards in time, without any need to store all of $a(t), t\in[0,T]$ at the same time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The difference between the adjoint method and backprop is that backprop stores the results of the intermediate steps $x_0,x_1,\ldots, x_{n-1}$, whereas the adjoint method recomputes them backwards. So the adjoint method consumes less memory (though I don’t know enough to say whether this is actually a problem that needs a solution).&lt;/p&gt;

&lt;h2 id=&quot;implementation-on-mnist-dataset&quot;&gt;Implementation on MNIST dataset&lt;/h2&gt;

&lt;p&gt;We will write a simple PyTorch implementation of an ODENet on the MNIST dataset using Ricky Chen’s &lt;a href=&quot;https://github.com/rtqichen/torchdiffeq&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torchdiffeq&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;First install &lt;code class=&quot;highlighter-rouge&quot;&gt;torchdiffeq&lt;/code&gt; using pip with the command&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install torchdiffeq
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let’s begin the script. Import all the relevant packages and prepare the MNIST dataset.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# enables the adjoint method for training,
# probably doesn't help in this setting
from torchdiffeq import odeint_adjoint as odeint

import matplotlib.pyplot as plt 
import numpy as np
import time

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize(0.5, 0.5)]) # normalize to [-1,1]

trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,
                                          shuffle=True, num_workers=4)

testset = torchvision.datasets.MNIST(root='./data', train=False,
                                       download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=16,
                                         shuffle=True, num_workers=4)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we define a class &lt;code class=&quot;highlighter-rouge&quot;&gt;MyNet&lt;/code&gt; under which we will write our two models.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MyNet(nn.Module):
	def __init__(self, path):
		super(MyNet, self).__init__()
		self.path = path

	def num_params(self):
		return sum(p.numel() for p in self.parameters() if p.requires_grad)

	def load(self):
		self.load_state_dict(torch.load('./' + self.path + '.pth'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we define the function $f(x,a,t)$ in the ODE $\dot{x} = f(x,a,t)$. For simplicity, we get rid of the $t$ dependence and make the control also time-independent. Below is the function ++f = \text{gn} \circ \text{conv}(a) \circ \text{relu} \circ \text{gn},++ where $\text{gn}$ is the GroupNorm function, $\text{relu}$ is the ReLU function, $\text{conv}$ is a convolution, and $a$ is the set of parameters of the $\text{conv}$ function. (In order to run this on a personal computer, we need a pretty small dimension so the GroupNorm will just become an InstanceNorm for us.) Notice that every function preserves dimension, which is necessary for the ODE to be defined.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class ODEFunc(nn.Module):
	def __init__(self, dim):
		super(ODEFunc, self).__init__()
		self.gn = nn.GroupNorm(min(32, dim), dim)
		self.conv = nn.Conv2d(dim, dim, 3, padding = 1)
		self.nfe = 0 # time counter

	def forward(self, t, x):
		self.nfe += 1
		x = self.gn(x)
		x = F.relu(x)
		x = self.conv(x)
		x = self.gn(x)
		return x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now we must define the ODEBlock which will replace the residual block. What you see below is the map $x_0 \to x(1)$ where $x$ is the solution of the differential equation
++ \dot{x}(t) = f(x(t), a), \quad t\in[0,1], \quad x(0) = x_0 ++ where $a$ is the set of parameters for &lt;code class=&quot;highlighter-rouge&quot;&gt;conv&lt;/code&gt;. More precisely, 
++ \dot{x}(t) = \text{gn} \circ \text{conv}(a) \circ \text{relu} \circ \text{gn}(x(t)), \quad t\in[0,1], \quad x(0)=x_0 ++ where all the parameters $a$ are coming from the convolution.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class ODEBlock(nn.Module):
	def __init__(self, odefunc):
		super(ODEBlock, self).__init__()
		self.odefunc = odefunc
		self.integration_time = torch.tensor([0, 1]).float()

	def forward(self, x):
		out = odeint(self.odefunc, x, self.integration_time, rtol=1e-1, atol=1e-1) # high tolerances for speed

		# first dimension is x(0) and second is x(1),
		# so we just want the second
		return out[1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we create a ODENet with this block. There are three parts to this ODENet.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;We take our 28-by-28 image and apply a 3-by-3 convolution without padding to it with 6 output channels. Then we apply GroupNorm and ReLU.&lt;/li&gt;
  &lt;li&gt;We apply the ODEBlock.&lt;/li&gt;
  &lt;li&gt;We apply a 2-by-2 average pool and one fully connected linear layer.
I keep track of the image dimensions below.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class ODENet(MyNet):
	def __init__(self):
		super(ODENet, self).__init__('mnist_odenet')
		self.conv1 = nn.Conv2d(1, 6, 3)
		self.gn = nn.GroupNorm(6, 6)
		self.odefunc = ODEFunc(6)
		self.odeblock = ODEBlock(self.odefunc)
		self.pool = nn.AvgPool2d(2)
		self.fc = nn.Linear(6 * 13 * 13, 10)

	def forward(self, x):
		# 26 x 26
		x = self.conv1(x)
		x = F.relu(self.gn(x))

		# stays 26 x 26
		x = self.odeblock(x)

		# 13 x 13
		x = self.pool(x)

		# fully connected layer
		x = x.view(-1, 6*13*13)
		x = self.fc(x)

		return x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s all! Now we just define the training and testing methods.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def train(net):
	n = 60000 / (5*16) # batch size 16
	criterion = nn.CrossEntropyLoss()
	optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

	for epoch in range(10):  
		running_loss = 0.0
		for i, data in enumerate(trainloader, 0):
			# get the inputs; data is a list of [inputs, labels]
			inputs, labels = data

			# zero the parameter gradients
			optimizer.zero_grad()

			# forward + backward + optimize
			outputs = net(inputs)
			loss = criterion(outputs, labels)
			loss.backward()
			optimizer.step()

			# print statistics
			running_loss += loss.item()
			if i % n == n-1:    
				print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / n))
				running_loss = 0.0

	print('Finished Training')
	torch.save(net.state_dict(), './' + net.path + '.pth')

def test(net):
	initial_time = time.time()
	correct = 0
	total = 0
	with torch.no_grad():
		for data in testloader:
			images, labels = data
			batch_size = images.shape[0]
			outputs = net(images)
			_, predicted = torch.max(outputs.data, 1)
			total += labels.size(0)
			correct += (predicted == labels).sum().item()
	final_time = time.time()
	print('Accuracy of the ' + net.path + ' network on the test set: %.2f %%' % (100 * correct / total))
	print('Time: %.2f seconds' % (final_time - initial_time))
	return(100 * correct / total)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s train our model. &lt;strong&gt;Note: this is pretty expensive to run on a personal computer. It took me half a day to train. If you’re doing this on a personal computer without a GPU, then consider making the ODENet even simpler, say with 2 channels instead of 6.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;odenet = ODENet()
train(odenet)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At this point, we have saved our parameters so we can load them back up for the test.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;odenet.load()
test(odenet)

# Output:
# Accuracy of the mnist_odenet network on the test set: 98.62 %
# Time: 44.61 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As you can see, this is very expensive just to test on a regular computer.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;A Neural ODE is the continuous analogue of a resnet. The main advantage is that in this setting, there is an alternative way to compute the parameter gradients using less memory.&lt;/p&gt;

&lt;p&gt;Some thoughts: during my short time reading up on this subject, I am not yet convinced that Neural ODEs provide enough of a performance boost to justify the additional computation required for it. First, I’m sure that there is a discrete analogue of the adjoint method which is almost surely simpler to implement than its continuous version, for applications in which memory usage is critical. Second, I’m not sure that there is a reason why the continuous model is a more accurate reflection of the true classifier than the discrete model. Usually in math, when there is a discrete model and an analogous continuous model, the continuous one is derived from a microscopic model, and the discrete model is a crude approximation of the continuous one. In such a setting, it’s clear that the continuous model is closer to the truth than the discrete model. In the Neural ODE setting, it’s not so clear.&lt;/p&gt;

&lt;p&gt;Below is the full code in one chunk.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# enables the adjoint method for training
from torchdiffeq import odeint_adjoint as odeint

import matplotlib.pyplot as plt 
import numpy as np
import time

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize(0.5, 0.5)]) # normalize to [-1,1]

trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,
                                          shuffle=True, num_workers=4)

testset = torchvision.datasets.MNIST(root='./data', train=False,
                                       download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=16,
                                         shuffle=True, num_workers=4)

############################################################################

class ODEFunc(nn.Module):
	def __init__(self, dim):
		super(ODEFunc, self).__init__()
		self.gn = nn.GroupNorm(min(32, dim), dim)
		self.conv = nn.Conv2d(dim, dim, 3, padding = 1)
		self.nfe = 0 # time counter

	def forward(self, t, x):
		self.nfe += 1
		x = self.gn(x)
		x = F.relu(x)
		x = self.conv(x)
		x = self.gn(x)
		return x

############################################################################

class ODEBlock(nn.Module):
	def __init__(self, odefunc):
		super(ODEBlock, self).__init__()
		self.odefunc = odefunc
		self.integration_time = torch.tensor([0, 1]).float()

	def forward(self, x):
		out = odeint(self.odefunc, x, self.integration_time, rtol=1e-1, atol=1e-1) # high tolerances for speed

		# first dimension is x(0) and second is x(1),
		# so we just want the second
		return out[1]

############################################################################

class ODENet(MyNet):
	def __init__(self):
		super(ODENet, self).__init__('mnist_odenet')
		self.conv1 = nn.Conv2d(1, 6, 3)
		self.gn = nn.GroupNorm(6, 6)
		self.odefunc = ODEFunc(6)
		self.odeblock = ODEBlock(self.odefunc)
		self.pool = nn.AvgPool2d(2)
		self.fc = nn.Linear(6 * 13 * 13, 10)

	def forward(self, x):
		# 26 x 26
		x = self.conv1(x)
		x = F.relu(self.gn(x))

		# stays 26 x 26
		x = self.odeblock(x)

		# 13 x 13
		x = self.pool(x)

		# fully connected layer
		x = x.view(-1, 6*13*13)
		x = self.fc(x)

		return x

############################################################################

def train(net):
	n = 60000 / (5*16) # batch size 16
	criterion = nn.CrossEntropyLoss()
	optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

	for epoch in range(10):  
		running_loss = 0.0
		for i, data in enumerate(trainloader, 0):
			# get the inputs; data is a list of [inputs, labels]
			inputs, labels = data

			# zero the parameter gradients
			optimizer.zero_grad()

			# forward + backward + optimize
			outputs = net(inputs)
			loss = criterion(outputs, labels)
			loss.backward()
			optimizer.step()

			# print statistics
			running_loss += loss.item()
			if i % n == n-1:    
				print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / n))
				running_loss = 0.0

	print('Finished Training')
	torch.save(net.state_dict(), './' + net.path + '.pth')

def test(net):
	initial_time = time.time()
	correct = 0
	total = 0
	with torch.no_grad():
		for data in testloader:
			images, labels = data
			batch_size = images.shape[0]
			outputs = net(images)
			_, predicted = torch.max(outputs.data, 1)
			total += labels.size(0)
			correct += (predicted == labels).sum().item()
	final_time = time.time()
	print('Accuracy of the ' + net.path + ' network on the test set: %.2f %%' % (100 * correct / total))
	print('Time: %.2f seconds' % (final_time - initial_time))
	return(100 * correct / total)


## Main ##
odenet = ODENet()
train(odenet)

## Test ##
odenet.load()
test(odenet)

# Output:
# Accuracy of the mnist_odenet network on the test set: 98.62 %
# Time: 44.61 seconds

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><summary type="html">You might have heard some hype around neural ordinary differential equations (Neural ODEs). The idea of using a continuous process to model a discrete one is very fundamental in many parts of mathematics, so I was excited to learn about this stuff. Even though the code from the original paper is available online, I couldn’t find a simple high-level explanation + implementation of neural ODEs on a simple dataset. In this post, I’ll explain the idea behind and purported advantages of Neural ODEs and create a MNIST classifier using a Neural ODE.</summary></entry><entry><title type="html">A Generalized Elo System for Tennis Players, part 1</title><link href="http://localhost:4000/2020/07/07/tennis-1.html" rel="alternate" type="text/html" title="A Generalized Elo System for Tennis Players, part 1" /><published>2020-07-07T00:00:00-07:00</published><updated>2020-07-07T00:00:00-07:00</updated><id>http://localhost:4000/2020/07/07/tennis-1</id><content type="html" xml:base="http://localhost:4000/2020/07/07/tennis-1.html">&lt;p&gt;Here, we will create an Elo rating system for male tennis players and do some analysis on the choice of parameters. We find that a random choice of parameters actually does quite well, and that a wide variety of K-factor weight functions do a good job predicting the outcomes of tennis matches. In future notebooks, we will further expand upon this model by adding various features.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;All of the code I wrote for this project is &lt;a href=&quot;https://github.com/hongsuh7/tennis-elo/blob/master/elo1.ipynb&quot;&gt;here&lt;/a&gt;, with similar text.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;The Elo rating system is designed to assign ratings to players which reflect their true strength at the game. Players are assigned an initial rating of 1. Before a match between two players, the system outputs a probability that one wins. After the match, the system updates each player’s rating according to their previous ratings and the result of the match.&lt;/p&gt;

&lt;h3 id=&quot;elos-rules&quot;&gt;Elo’s rules&lt;/h3&gt;
&lt;p&gt;Define ++\sigma(x) = \exp(x) / (\exp(x) + 1),++ the logistic function, and let $K$ be a constant. If player one (p1) has rating $x$ and player two (p2) has rating $y$, the probability that p1 wins is given by $\sigma(x-y)$. Suppose $w=1$ if p1 wins and $w=0$ if p1 loses. After the match, the ratings are updated with the rule ++x \mapsto x + (-1)^{w+1} K\sigma((-1)^w(x-y)),\quad y \mapsto y+(-1)^w K\sigma((-1)^w (x-y)).++&lt;/p&gt;

&lt;h3 id=&quot;deriving-elos-rules&quot;&gt;Deriving Elo’s rules&lt;/h3&gt;
&lt;p&gt;Let’s see why this makes any sense. Suppose we are starting from scratch and we want to develop this kind of rating system, but have no idea what the win probability and the update rule should be. Heuristically, it makes sense to suppose that $W,U$ are functions of the difference $x-y$ rather than the separate ratings $x,y$. Let $W(x-y)$ be the probability that p1 wins against p2, and let $U(x-y)$ be the update rule if p1 loses and $U(y-x)$ be the update rule if p1 wins, so that ++ x\mapsto x+(-1)^{w+1} U((-1)^w(x-y)),\quad y\mapsto y+(-1)^w U((-1)^w(x-y)).++ We need $W$ and $U$ to satisfy some basic rules.&lt;/p&gt;

&lt;p&gt;First, $W(x-y) = 1-W(y-x)$ so ++W(z)+W(-z)=1.++&lt;/p&gt;

&lt;p&gt;Second, ++\lim_{x\to \infty} W(x)=1 ~\text{ and }~ \lim_{x\to-\infty} U(x) = 0.++&lt;/p&gt;

&lt;p&gt;Third and last, &lt;em&gt;the expected update of both players must be zero&lt;/em&gt;. This is because the strengths of the players shouldn’t actually change after a match, so the Elo rating (which is supposed to reflect true strength) shouldn’t do that. Here, $w$ is the random quantity in question (make sure to distinguish between little $w$ and capital $W$: the relationship is that $W(x-y)$ should estimate $P(w=1)$). Since the expected update of both players equals zero, we have ++ W(x-y) \cdot U(y-x) - W(y-x) \cdot U(x-y) = 0. ++ In other words, ++ \frac{W(z)}{W(-z)} = \frac{U(z)}{U(-z)}.++
In fact, these are the only real mathematical requirements for such a rating system. (See &lt;a href=&quot;https://www.stat.berkeley.edu/~aldous/Papers/me-Elo-SS.pdf&quot;&gt;Aldous&lt;/a&gt; for more details and assumptions.) As we see, an easy choice would be to set $KW=U$, where $K$ is any constant greater than zero. And really we may set $W$ to be any cdf, but logistic is the standard choice.&lt;/p&gt;

&lt;p&gt;In practice, the choice of $K$ is quite important, and the constant is called the &lt;em&gt;K-factor&lt;/em&gt;, not to be confused with the tennis racquet line. This $K$ is the main subject of this note, and we will refer to it many times.&lt;/p&gt;

&lt;p&gt;Generally $K$ is taken to be a decreasing function of the number of matches that the updating player has played. In our case, following FiveThirtyEight’s family of functions for $K$, we take ++K(n) = \frac{a}{(b+n)^c},++ where $n$ is the number of matches played by that player before the match. We will call $K(n)$ the &lt;strong&gt;K-factor function&lt;/strong&gt;. All of the analysis of parameters we do below focus on the parameters $a,b,c$ above.&lt;/p&gt;

&lt;p&gt;We will use &lt;a href=&quot;http://jeffsackmann.com&quot;&gt;Jeff Sackmann&lt;/a&gt;’s immense trove of tennis data; see his &lt;a href=&quot;https://github.com/JeffSackmann&quot;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;differences-with-standard-elo-ratings&quot;&gt;Differences with standard Elo ratings&lt;/h3&gt;
&lt;p&gt;You may notice that I chose to work with the natural base rather than base-10 with some weird 400 multiplicative factor like with standard Elo ratings. The only difference between my ratings and the ratings you may see on FiveThirtyEight or Jeff Sackmann’s website is a multiplicative factor of $400/\ln 10\approx 173.72$. There is also an additive factor, but we can ignore that because Elo ratings give the same predictions when the same value is added to all ratings (and all initial ratings). We explain the origin of the multiplicative factor below.&lt;/p&gt;

&lt;p&gt;For the standard Elo rating, the win probability of player with rating $x$ (who is facing a player with rating $y$) is ++ \frac{1}{1+10^{(y-x)/400}} = \frac{1}{1+ \exp\left(\frac{\ln 10}{400}(y-x)\right)} ++
So we have to multiply our ratings by $400/\ln 10\approx 173.72$ to get the standard Elo ratings. There’s also an additive factor of $1000-173.72$ because generally the tennis ratings start at 1000, and we start at 1.&lt;/p&gt;

&lt;h2 id=&quot;cost-functions&quot;&gt;Cost functions&lt;/h2&gt;
&lt;p&gt;There are many ways to measure the accuracy of a prediction model which outputs probabilities. Let $n$ be the number of matches being analyzed and $p_i$ the win probability that the model assigns to the winner for $i=1,2,\ldots,n$.&lt;/p&gt;

&lt;p&gt;The easiest to understand is &lt;em&gt;win prediction rate&lt;/em&gt;, which is simply the proportion of matches for which the model assigns a probability greater than 0.5 to the winner. In the code, win prediction rate is denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt;. Below, $\mathbb{1}(A)=1$ if $A$ happens and $\mathbb{1}(A)=0$ if $A$ does not happen. Technically this is not a cost function, it’s a profit function. Take the negative of this if you want a cost function.
++\text{wp} = n^{-1}\sum_{i=1}^n \mathbb{1}\{p_i &amp;gt; 0.5\}.++&lt;/p&gt;

&lt;p&gt;Next, we introduce &lt;em&gt;log-loss error&lt;/em&gt;, appearing in maximum likelihood estimation and logistic regression. In the code, log-loss error is denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;.
++\text{ll} = -n^{-1}\sum_{i=1}^n \log p_i.++&lt;/p&gt;

&lt;p&gt;Finally, we introduce the &lt;em&gt;Brier score&lt;/em&gt;, which is simply the mean squared error between $p_i, i=1,2,\ldots,n$ and the perfect data which assigns probability one to the winner every time. In the code, Brier score is denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;.
++\text{bs} = n^{-1} \sum_{i=1}^n (1-p_i)^2.++&lt;/p&gt;

&lt;p&gt;Here are some apparent differences between these cost/profit functions.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;win prediction rate is not continuous (a small change in probabilities can change 0s to 1s and 1s to 0s) and also flat in many places (with respect to the parameters), so it is less useful for optimization than the other two functions, though still a nice metric.&lt;/li&gt;
  &lt;li&gt;log-loss is not bounded, while Brier score is. To see the difference, suppose $p_1=10^{-100}$. Then this contributes 230 to the log-loss error, while it contributes 1 to the Brier score. So log-loss is less tolerant of wrongly confident predictions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s first look at FiveThirtyEight’s model, described in a very nice article about &lt;a href=&quot;https://fivethirtyeight.com/features/serena-williams-and-the-difference-between-all-time-great-and-greatest-of-all-time/&quot;&gt;Serena Williams’ dominance over the years&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fivethirtyeights-model&quot;&gt;FiveThirtyEight’s model&lt;/h2&gt;
&lt;p&gt;FiveThirtyEight’s parameter choices were ++[a,b,c]=[250/174, 5, 0.4] \approx [1.44, 5, 0.4].++ Let’s test their model on 2014 data with one year of history and 34 years of history.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;One year of history:
log-loss=0.601, win pred=0.650, brier score=0.209
many years of history:
log-loss=0.585, win pred=0.669, brier score=0.201
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I got the same log-loss error that &lt;a href=&quot;https://core.ac.uk/download/pdf/86638968.pdf&quot;&gt;Kovalchik&lt;/a&gt; got, but I got a different win prediction. I’m not entirely certain how that happened, since I think we used the same data set. I’m gonna have to go on with what I have.&lt;/p&gt;

&lt;h2 id=&quot;sensitivity-of-ratings&quot;&gt;Sensitivity of Ratings&lt;/h2&gt;
&lt;p&gt;Recall that the parameters we are interested in are the ones which dictate the behavior of the &lt;em&gt;K-factor function&lt;/em&gt;. My main observation for this post is that ratings are not very sensitive to the parameters because 1) the model itself is pretty robust, and 2) there are some redundancies in our three parameters. By this, I mean that very different sets of parameters can produce similar K-factor functions.&lt;/p&gt;

&lt;p&gt;We will initialize an object of class Elo with parameters $p$ which are obtained from the following distribution:&lt;/p&gt;

&lt;p&gt;++ p \sim \text{Unif}([0,2]\times[2,5]\times[0,1]). ++&lt;/p&gt;

&lt;p&gt;We will calculate and plot log-loss error, win prediction rate, and Brier score for 100 models with random parameters drawn from the above distribution. To do this fast, we need to modify our class to accommodate &lt;strong&gt;vectorized&lt;/strong&gt; operations. We change our code so that ratings according to different parameters are all updated simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tennis-1/ll.png&quot; alt=&quot;1&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/wp.png&quot; alt=&quot;2&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/bs.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The conclusion is that &lt;strong&gt;most parameters drawn from this simple uniform distribution do pretty well.&lt;/strong&gt; The parameter population is the densest where the best costs are achieved. In other words, good parameters are not rare. Recall that log-loss=0.585, win pred=0.669, brier score=0.201 were the costs achieved by the handpicked FiveThirtyEight parameters. The plots have vertical lines at these checkpoints.&lt;/p&gt;

&lt;h2 id=&quot;you-bring-your-best-fifty-ill-bring-mine&quot;&gt;You bring your best fifty, I’ll bring mine&lt;/h2&gt;
&lt;p&gt;How good are the best parameters from these randomly chosen ones? Let’s pick the top 1% of parameters and see how they perform. I will pick the 50 best parameters (tested on 2010-2013) in each of the three categories from 5000 random ones with 1990-2013 data. Then we test them on 2014 data. For each category, to get a single probability, we took the 50 probabilities generated by the 50 models and averaged them.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  optimized_for        ll        wp        bs
0            ll  0.581776  0.670516  0.199910
1            wp  0.595646  0.666440  0.204376
2            bs  0.581727  0.669497  0.199868
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that the parameters which were obtained by optimizing for log-loss and Brier score were about equally effective at predicting 2014 matches, and the win-prediction-optimized parameters were less effective. But overall, all of these parameters are &lt;em&gt;not bad&lt;/em&gt;. What do these parameters look like? Are they clustered anywhere? Are they all over the place? Recall the roles of the first, second, and third parameters, below denoted by $a,b,c$: ++k(n) = \frac{a}{(b+n)^c}.++&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tennis-1/ab.png&quot; alt=&quot;4&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/ac.png&quot; alt=&quot;5&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/bc.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From these plots, we can see that basically b doesn’t matter, and based on the type of error optimized for, the parameters cluster around different pairs of $(a,c)$ values. There is some overlap between the log-loss optimized parameters and the Brier-score optimized parameters, as evidenced by the purple points.&lt;/p&gt;

&lt;p&gt;The correlation in the second plot is due to the fact that, for a fixed $n,a,b,$ and $c$, the set of points $(x,y)$ satisfying ++\frac{x}{(b+n)^y} = \frac{a}{(b+n)^c}++ is a log curve.&lt;/p&gt;

&lt;p&gt;In short, &lt;strong&gt;there are a variety of parameter choices that can lead to effective predictions.&lt;/strong&gt; All of the parameters shown above achieve pretty good error rates.&lt;/p&gt;

&lt;p&gt;You may be wondering if maybe, two sets of parameters could be quite different as points in 3-d space but give rise to two very similar k-factor functions $k(n)$. This is true, as evidenced by the clustering around a log curve shown above. Our tentative conclusion is that parameters within a single group (that is, optimized for the same cost function) give rise to similar K-factor functions, but the three different groups (&lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;-optimized,&lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt;-optimized,&lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;-optimized) give rise to fairly different K-factor functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tennis-1/k.png&quot; alt=&quot;7&quot; /&gt;
&lt;img src=&quot;/assets/tennis-1/k2.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems clear that the differently colored K-factor functions behave quite differently, both near zero and at infinity.&lt;/p&gt;

&lt;h2 id=&quot;testing-on-2015-2019-data&quot;&gt;Testing on 2015-2019 data&lt;/h2&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;-optimized and &lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;-optimized parameters (trained on 1980-2013) performed a tiny better than the FiveThirtyEight model with respect to each of the three cost functions for 2014 data. What about for 2015-2019 data? We’ll check it out here.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  optimized_for        ll        wp        bs
0            ll  0.607112  0.658860  0.209942
1            wp  0.623405  0.657555  0.214121
2            bs  0.607411  0.658723  0.209912
3           538  0.611903  0.661607  0.210791
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;FiveThirtyEight takes the highest win prediction rate. On the other hand, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;- and &lt;code class=&quot;highlighter-rouge&quot;&gt;bs&lt;/code&gt;-optimized parameters achieve the best log-loss errors. Overall, aside from &lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt;-optimized parameters, all of these models seem to give roughly equally effective predictions. (This discrepancy is explained by the fact that mathematically, &lt;code class=&quot;highlighter-rouge&quot;&gt;wp&lt;/code&gt; is not nice to work with because it is flat and discontinuous.)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;First, this analysis casts doubt on my initial feeling that there is one “true” K-factor function that we wish to approximate. It seems that many quite different K-factor functions can be used for effective and near-optimal prediction, given this model.&lt;/p&gt;

&lt;p&gt;Second, I believe that a random choice of parameters is a better one than a handpicked one, from an algorithmic point of view. It separates the parameters from most human fiddling (except the distribution that we pick from, which was pretty generic – a uniform distribution on a large 3-d box).&lt;/p&gt;

&lt;p&gt;Third, the fact that many different K-factor functions work opens up a new frontier of prediction with Elo. Can we use, say, ten different Elo models at once to make a better prediction than any one of them can? Perhaps we can use a fast-updating Elo model in conjunction with a slow-updating Elo model to generate more effective predictions. We will explore these ideas and more in the next notebook.&lt;/p&gt;</content><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><summary type="html">Here, we will create an Elo rating system for male tennis players and do some analysis on the choice of parameters. We find that a random choice of parameters actually does quite well, and that a wide variety of K-factor weight functions do a good job predicting the outcomes of tennis matches. In future notebooks, we will further expand upon this model by adding various features.</summary></entry><entry><title type="html">Simulated Annealing and Smitten Ice Cream</title><link href="http://localhost:4000/2020/06/29/smitten.html" rel="alternate" type="text/html" title="Simulated Annealing and Smitten Ice Cream" /><published>2020-06-29T00:00:00-07:00</published><updated>2020-06-29T00:00:00-07:00</updated><id>http://localhost:4000/2020/06/29/smitten</id><content type="html" xml:base="http://localhost:4000/2020/06/29/smitten.html">&lt;p&gt;I live in Oakland, about a mile away from a &lt;a href=&quot;https://www.smittenicecream.com&quot;&gt;Smitten Ice Cream&lt;/a&gt; store. Their selling point is their super-fast liquid nitrogen made-to-order ice cream. They claim that the ice cream, which is turned solid from liquid in 90 seconds, is creamier than regular ice cream. The validity of the scientific basis of this claim, I can’t answer, but I can make a simple mathematical model, derived from physical principles, to simulate the comparison between Smitten-made ice cream and regular ice cream.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Of course, I wasn’t really thinking about Smitten when I was learning this stuff (I prefer &lt;a href=&quot;https://www.curbsideoakland.com&quot;&gt;Curbside&lt;/a&gt;). I was looking for ways to optimize a pretty rough function that was not really suitable for any gradient techniques, and I wanted to do something a little more sophisticated than literally a random search over some portion of the parameter space (which ended up being the best option, I’ll post about this later). A friend told me about &lt;a href=&quot;https://en.wikipedia.org/wiki/Simulated_annealing&quot;&gt;simulated annealing&lt;/a&gt;, which seemed right up my alley because I did a little statistical mechanics in grad school. It ended up not working really well for my problem, but I did find this topic interesting so I thought I’d write about it. &lt;strong&gt;My goal in this post is to explain a simple model for flash freezing using simulated annealing.&lt;/strong&gt; I was inspired by the image on the Wikipedia page, which you should compare to the images in this post.&lt;/p&gt;

&lt;h2 id=&quot;particle-configurations&quot;&gt;Particle Configurations&lt;/h2&gt;

&lt;p&gt;Let’s consider an arrangement of two types of particles in a box in a $n\times n$ two-dimensional lattice, like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine that these particles form the liquid that Smitten ice cream is made from. Of course, the actual Smitten Goo in all probably has millions of different kinds of particles, not arranged in a grid, in three-dimensional space, whatever. Our purpose here is to simplify the very complicated reality of ice cream as much as we can until we arrive at the simplest model which demonstrates the essence of our question: &lt;em&gt;creaminess&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To do this, we only need two types of particles. We’ll assign red particles to the value $+1$ and blue particles to the value $-1$. We will often say that red particles have &lt;em&gt;positive spin&lt;/em&gt; and blue particles have &lt;em&gt;negative spin&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The entire arrangement of particles is called a &lt;em&gt;particle configuration&lt;/em&gt;. To reiterate and simplify, a particle configuration consists of an assignment of $+1$ or $-1$ to each site in the $n\times n$ box. We will denote the collection of all particles configurations by $B_n$. So each element in $B_n$ is a different particle configuration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;energy-and-temperature&quot;&gt;Energy and temperature&lt;/h2&gt;

&lt;p&gt;Energy and temperature are two different quantities which work against, and for, one another to accomplish the task of freezing, whcih we are interested in because, remember, we love ice cream.&lt;/p&gt;

&lt;h3 id=&quot;energy&quot;&gt;Energy&lt;/h3&gt;

&lt;p&gt;Here is the part where we invoke some physical principles. Each particle configuration has an energy associated to it. The details of energy are not pressing right now, so we will defer this discussion to the appendix. You can invent any notion of energy you’d like. We describe one type of energy, &lt;em&gt;attractive energy&lt;/em&gt;, below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Particles want to position themselves in a low-energy configuration. This is a general physical principle, the Second Law of Thermodynamics. In our model, their ability to do so depends on one parameter, &lt;em&gt;temperature&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;temperature&quot;&gt;Temperature&lt;/h3&gt;

&lt;p&gt;Here is another physically guided principle, though I’m not sure what the following tendency is called. At high temperatures, particles are more tolerant of high-energy configurations. As the temperature decreases, particles develop a more urgent need to have low energy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-simulated-annealing-algorithm&quot;&gt;The simulated annealing algorithm&lt;/h3&gt;

&lt;p&gt;So far, we have described what particles would like to do, but we haven’t specified a process with which they can accomplish their desires. We will begin to describe the simulated annealing algorithm, which does exactly this. Essentially, &lt;strong&gt;simulated annealing simulates the movement of particles in the process of freezing.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To simulate the process of freezing, we let a particle configuration evolve over time in the following way. At each time interval, one particle is randomly chosen. Then it either switches its spin or keeps its spin, depending on the &lt;strong&gt;energy&lt;/strong&gt; of the configuration and the &lt;strong&gt;temperature&lt;/strong&gt; of the environment.&lt;/p&gt;

&lt;p&gt;Here is the qualitative evolution rule:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Begin with a random initial configuration.&lt;/li&gt;
  &lt;li&gt;Record the energy $E$.&lt;/li&gt;
  &lt;li&gt;At each time interval, choose a random particle, switch its spin, and recompute the energy $E’$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The particle configuration either accepts or declines the change. The choice is random, with tendencies listed below.&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt; &lt;/th&gt;
          &lt;th&gt;high temperature&lt;/th&gt;
          &lt;th&gt;low temperature&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$E&amp;gt;E’$&lt;/td&gt;
          &lt;td&gt;slight inclination to accept&lt;/td&gt;
          &lt;td&gt;strong inclination to accept&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;$E&amp;lt;E’$&lt;/td&gt;
          &lt;td&gt;slight inclination to decline&lt;/td&gt;
          &lt;td&gt;strong inclination to decline&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;Once the change is either accepted or declined, the temperature decreases by some amount.&lt;/li&gt;
  &lt;li&gt;Repeat steps 2 through 5 for a fixed number of iterations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is not the exact physical process by which freezing of liquids happens. This is just a simple mathematical model which captures the creaminess property that we wish to investigate. This is the simulated annealing algorithm. It was invented for the sake of finding global extrema of a general class of functions. But that’s not what we’re using it for today.&lt;/p&gt;

&lt;h2 id=&quot;flash-freezing&quot;&gt;Flash Freezing&lt;/h2&gt;

&lt;p&gt;Here’s the interesting part. &lt;strong&gt;The speed of temperature decrease affects the microscopic structure of the particle configuration&lt;/strong&gt;. If we compare two simulated annealing processes with different temperature decrease speeds and identical initial configurations, initial temperatures, final temperatures, and number of iterations, then the slow-cooled configuration will have more ‘‘order’’ than the fast-cooled configuration. We will see different examples of this phenomenon below. Below, we see slow-cooled configurations and fast-cooled configurations with identical parameters otherwise, for two different notions of energy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/6_3.png&quot; alt=&quot;6&quot; /&gt;
&lt;img src=&quot;/assets/smitten-1/7_2.png&quot; alt=&quot;7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the first example, slow cooling results in larger groups of same-spin particles than fast cooling does. In the second example, slow cooling results in a more crystalline arrangement of particles than fast cooling does. This is what I mean when I say that slow cooling produces more order than fast cooling. The reason is that generally, more order means less energy, and slow cooling results in a lower final energy than fast cooling does. (When using simulated annealing for an optimization problem, fast cooling will find a local minimum close to the starting point, and slow cooling will get you closer to the global minimum.)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The freezing of ice cream is a much more complicated physical process than the simple model described above. However, this simple model captures many aspects of the ice cream freezing process. In particular, we see that a slow freezing process results in a crystalline structure while a fast freezing process results in a disordered liquidlike structure. Ice crystal formation follows a similar energy principle to the one described above, and fast freezing an ice cream goo is sure to generate smaller crystals, which are more disorderly, than slow freezing. See the image below to see what flash freezing does to food (?) cells (obtained from flash-freeze.net):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/smitten-1/cells.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The flash-frozen cells retain cell structure because the ice crystals are smaller and less crystalline, which results in a smaller volume expansion (remember that ice is less dense than water), which prevents cells from exploding. Overall, flash-freezing a liquid keeps it more similar to the original liquid than slow-freezing it does. So there is a mathematical basis to Smitten’s claim that their 90-second liquid-nitrogen-induced ice cream making process produces smaller ice crystals and therefore a creamier texture than other ice creams.&lt;/p&gt;

&lt;p&gt;To be clear, though, I don’t really detect any creaminess difference between Smitten and other ice creams. Especially when a Smitten pint is eleven dollars…&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;p&gt;I considered periodic grids, so that neighbors are easy to calculate. The energy I used for the first simulation above is a standard Ising model energy
++ E(x) = -\sum_{\|i-j\|_{\infty} = 1} x_i x_j ++ where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$i,j \in \mathbb{Z}^2$;&lt;/li&gt;
  &lt;li&gt;$x_i \in \{+1,-1\}$ is the spin at site $i$; and&lt;/li&gt;
  &lt;li&gt;$\|i-j\|_{\infty}$ is the larger of the vertical and horizontal distances between $i$ and $j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The energy I used for the second simulation includes a repulsive energy at larger distances&lt;/p&gt;

&lt;p&gt;++ E(x) = -\sum_{\|i-j\|_{\infty} = 1} x_i x_j + \frac{1}{2} \sum_{\|i-j\|_\infty=3} x_i x_j. ++&lt;/p&gt;

&lt;p&gt;The temperature decrease speed I used was of the following shape, with speed parameter $\alpha$
++ T(\alpha, t) = 4(1-t)^\alpha + 1, ++ where $t$ goes from zero to one. The slow cooling implemented a linear decrease with $\alpha=1$ and the parameter for fast cooling was $\alpha = 10$.&lt;/p&gt;

&lt;p&gt;A few more details:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the energies you should normalize by dividing by the size of the grid;&lt;/li&gt;
  &lt;li&gt;the images are 100 by 100;&lt;/li&gt;
  &lt;li&gt;the first pair of images went through 300000 time steps and the second pair through 100000 time steps;&lt;/li&gt;
  &lt;li&gt;this takes impossibly long if you recalculate the entire energy after a sign flip, so you have to calculate the local change to update the energy;&lt;/li&gt;
  &lt;li&gt;the switch-acceptance probability was ++ P(E, E’, T) = \frac{1}{Z}\exp((E - E’)/ T), ++ where $E$ is old energy, $E’$ is new energy, $T\in [0,1]$ is temperature, and $Z$ is a normalizing constant which I set to be one. Note that this quantity, though it is called a probability, is not exactly a probability because it may be larger than one. Just cut it off at one.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

Since this is the first post, suffice it to say this: we will pick a particular probability distribution on $B_n$ such that *lower-energy particle configurations have higher probability of occurring*. 

Furthermore, the probability distribution has a parameter $\beta$ between 0 and infinity. Increasing $\beta$ makes lower-energy particle configurations **even more likely**. Decreasing $\beta$ makes lower-energy particle configurations **a little less likely than they were with a higher $\beta$**.
--&gt;

&lt;!-- next up let's apply simulated annealing to find least-cost path across potentials
http://eprints.qut.edu.au/62208/1/MiaoTianV11withPubInfo.pdf --&gt;</content><author><name>Hong Suh</name><email>hong.suh7@gmail.com</email></author><summary type="html">I live in Oakland, about a mile away from a Smitten Ice Cream store. Their selling point is their super-fast liquid nitrogen made-to-order ice cream. They claim that the ice cream, which is turned solid from liquid in 90 seconds, is creamier than regular ice cream. The validity of the scientific basis of this claim, I can’t answer, but I can make a simple mathematical model, derived from physical principles, to simulate the comparison between Smitten-made ice cream and regular ice cream.</summary></entry></feed>